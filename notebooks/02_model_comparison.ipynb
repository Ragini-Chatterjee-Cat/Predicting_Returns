{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volatility Model Comparison\n",
    "\n",
    "This notebook compares different volatility forecasting models using out-of-sample evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from data.fetcher import fetch_stock_data\n",
    "from data.preprocessing import calculate_returns\n",
    "from models.garch import GARCHModel, EGARCHModel, GJRGARCHModel\n",
    "from models.ml_models import RandomForestVolatility, XGBoostVolatility\n",
    "from models.stochastic_vol import HestonModel\n",
    "from evaluation.backtesting import RollingWindowBacktest\n",
    "from evaluation.metrics import calculate_all_metrics, diebold_mariano_test\n",
    "import config\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch SPY data for model comparison\n",
    "ticker = 'SPY'\n",
    "data = fetch_stock_data(\n",
    "    ticker=ticker,\n",
    "    start_date=config.START_DATE,\n",
    "    end_date=config.END_DATE\n",
    ")\n",
    "\n",
    "prices = data['close']\n",
    "returns = calculate_returns(prices.to_frame(), method='log').squeeze()\n",
    "\n",
    "print(f\"Data loaded: {len(returns)} observations\")\n",
    "print(f\"Date range: {returns.index[0]} to {returns.index[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all models\n",
    "models = {\n",
    "    'GARCH(1,1)': GARCHModel(p=1, q=1),\n",
    "    'EGARCH(1,1)': EGARCHModel(p=1, q=1),\n",
    "    'GJR-GARCH(1,1)': GJRGARCHModel(p=1, o=1, q=1),\n",
    "    'Random Forest': RandomForestVolatility(n_estimators=50, lookback=20),\n",
    "    'XGBoost': XGBoostVolatility(n_estimators=50, lookback=20),\n",
    "    'Heston': HestonModel()\n",
    "}\n",
    "\n",
    "print(\"Models initialized:\")\n",
    "for name in models:\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Single Model Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GARCH model on full sample\n",
    "garch = GARCHModel(p=1, q=1)\n",
    "garch.fit(returns)\n",
    "\n",
    "print(\"GARCH(1,1) Parameters:\")\n",
    "print(garch.params)\n",
    "print(f\"\\n5-day ahead variance forecast: {garch.forecast_variance(5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot conditional variance\n",
    "cond_var = garch.get_conditional_variance()\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "axes[0].plot(returns.index, returns, alpha=0.7, color='steelblue')\n",
    "axes[0].set_title(f'{ticker} Returns')\n",
    "axes[0].set_ylabel('Return')\n",
    "\n",
    "axes[1].plot(returns.index, np.sqrt(cond_var) * np.sqrt(252), color='darkred', linewidth=1.5)\n",
    "axes[1].set_title('GARCH(1,1) Conditional Volatility (Annualized)')\n",
    "axes[1].set_ylabel('Volatility')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rolling Window Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize backtesting framework\n",
    "backtest = RollingWindowBacktest(\n",
    "    train_window=config.TRAIN_WINDOW,\n",
    "    test_window=config.TEST_WINDOW,\n",
    "    step_size=config.TEST_WINDOW,\n",
    "    expanding=False\n",
    ")\n",
    "\n",
    "# Run backtest (this may take a while)\n",
    "print(\"Running backtest...\")\n",
    "results = backtest.run(returns, models, verbose=True)\n",
    "print(f\"\\nBacktest completed. Results shape: {results.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview results\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "metrics = backtest.evaluate(results)\n",
    "metrics.round(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank models by different metrics\n",
    "print(\"Model Rankings:\")\n",
    "print(\"=\"*50)\n",
    "for metric in ['mse', 'mae', 'qlike']:\n",
    "    ranking = metrics[metric].sort_values()\n",
    "    print(f\"\\n{metric.upper()}:\")\n",
    "    for i, (model, value) in enumerate(ranking.items(), 1):\n",
    "        print(f\"  {i}. {model}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot forecasts vs realized\n",
    "fig, ax = backtest.plot_forecasts(results, figsize=(14, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative loss\n",
    "fig, ax = backtest.plot_cumulative_loss(results, loss_func='qlike', figsize=(14, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics comparison bar plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax, metric in zip(axes, ['rmse', 'mae', 'qlike']):\n",
    "    values = metrics[metric].sort_values()\n",
    "    colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(values)))\n",
    "    values.plot(kind='barh', ax=ax, color=colors)\n",
    "    ax.set_title(metric.upper())\n",
    "    ax.set_xlabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diebold-Mariano test comparing all models to GARCH benchmark\n",
    "benchmark = 'GARCH(1,1)'\n",
    "realized = results['realized'].values\n",
    "\n",
    "print(f\"Diebold-Mariano Test vs {benchmark}:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<20} {'DM Stat':>12} {'p-value':>12} {'Winner':>15}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for model in models:\n",
    "    if model != benchmark:\n",
    "        pred1 = results[benchmark].values\n",
    "        pred2 = results[model].values\n",
    "        \n",
    "        # Remove NaN\n",
    "        valid = ~(np.isnan(pred1) | np.isnan(pred2) | np.isnan(realized))\n",
    "        \n",
    "        dm_stat, p_value = diebold_mariano_test(\n",
    "            realized[valid], pred1[valid], pred2[valid], loss_func='qlike'\n",
    "        )\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            winner = benchmark if dm_stat > 0 else model\n",
    "        else:\n",
    "            winner = \"No diff\"\n",
    "        \n",
    "        print(f\"{model:<20} {dm_stat:>12.4f} {p_value:>12.4f} {winner:>15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mincer-Zarnowitz regression results\n",
    "print(\"\\nMincer-Zarnowitz Regression Results:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<20} {'Alpha':>10} {'Beta':>10} {'RÂ²':>10} {'Joint p-val':>15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for model in models:\n",
    "    alpha = metrics.loc[model, 'mz_alpha']\n",
    "    beta = metrics.loc[model, 'mz_beta']\n",
    "    r2 = metrics.loc[model, 'mz_r_squared']\n",
    "    pval = metrics.loc[model, 'mz_joint_pvalue']\n",
    "    print(f\"{model:<20} {alpha:>10.6f} {beta:>10.4f} {r2:>10.4f} {pval:>15.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusions\n",
    "\n",
    "Key findings from the model comparison:\n",
    "\n",
    "1. **Best performing model**: Check metrics table above\n",
    "2. **GARCH vs ML**: Compare traditional econometric vs machine learning approaches\n",
    "3. **Asymmetric models**: EGARCH and GJR-GARCH capture leverage effects\n",
    "4. **Forecast efficiency**: Mincer-Zarnowitz tests reveal forecast bias/efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "summary = metrics[['rmse', 'mae', 'qlike', 'r_squared', 'mz_beta']].copy()\n",
    "summary['rank_qlike'] = summary['qlike'].rank()\n",
    "summary = summary.sort_values('rank_qlike')\n",
    "\n",
    "print(\"\\nFinal Model Ranking (by QLIKE):\")\n",
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
